# QuantumState

**Autonomous SRE agent swarm built on Elasticsearch. Detects production anomalies, traces root causes, executes remediations, and verifies recovery â€” fully closed loop, under 4 minutes.**

ğŸŒ [Live Demo](https://www.quantumstate.online) Â· ğŸ¤– [Agents Definition](agents-definition.md)

---

## The Problem

Imagine a backend service running in production. Memory usage starts climbing. At first, it looks harmless. Then it crosses a threshold. Latency spikes. Error rates rise. Alerts fire. At 3:00 AM, someone gets paged.

An SRE now has to:

- Check dashboards
- Query logs
- Correlate recent deployments
- Identify the root cause
- Decide on remediation (restart? rollback? scale?)
- Verify the system has recovered

Even with good observability, this process is manual, repetitive, and time-sensitive. MTTR increases not because data is unavailable â€” but because humans must interpret and act on it.

The real problem isn't detecting issues. It's turning detection into reliable, automated action.

---

## Introducing QuantumState

Most autonomous incident response systems work the same way: data flows out of your observability platform into an external AI layer, decisions get made somewhere else, and then actions are fired back through webhooks or APIs. You end up with external LLM API keys, custom orchestration middleware, fragile integrations â€” and your logs and metrics traveling across system boundaries on every incident.

QuantumState is built differently.

It is an autonomous SRE agent swarm built entirely on **[Elastic Agent Builder](https://www.elastic.co/docs/explore-analyze/ai-features/agent-builder/agent-builder-agents)** â€” Elastic's native framework for building and running AI agents directly inside your Elasticsearch cluster. Detection, investigation, remediation decisions, and verification all happen inside the platform where your data already lives. No data egress. No glue code. No external orchestration.

Four specialized agents run in sequence, each responsible for a distinct phase of the incident lifecycle:

1. **Detect** â€” Identify anomalies in metrics before they cascade.
2. **Investigate** â€” Correlate metrics and logs to determine the root cause.
3. **Execute** â€” Trigger a remediation action when confidence is high.
4. **Verify** â€” Confirm that system metrics have returned to baseline.

Instead of stopping at alerting, QuantumState carries the incident from detection to verified recovery â€” automatically, with a full audit trail written back to Elasticsearch at every step.

---

## Built on Elastic Agent Builder

**[Elastic Agent Builder](https://www.elastic.co/docs/explore-analyze/ai-features/agent-builder/agent-builder-agents)** lets you define agents â€” system prompts, tools, and workflow triggers â€” natively inside Kibana. The same configuration is fully accessible via the Kibana API for automation and CI/CD. For QuantumState, this means:

It lets you build agents directly inside the Elastic ecosystem â€” where your logs and metrics already live. Tools, system prompts, and workflow triggers are defined natively in Kibana with a clean UI, and the same configuration is fully accessible via the Kibana API for automation and CI/CD integration.

For QuantumState, this means:

- **No external LLM API keys** â€” agents run within your Elastic deployment
- **No orchestration middleware** â€” ES|QL queries are the agent tools; Elasticsearch is the reasoning substrate
- **No data egress** â€” detection, investigation, and remediation decision-making happen inside the cluster where the data lives
- **Full auditability** â€” every agent decision, tool call, and workflow trigger is written back to Elasticsearch

<img src="images/Elastic Agent Builder - Home.png" width="720" alt="Elastic Agent Builder Home" />

<img src="images/Elastic Agent Builder - New Agent.png" width="720" alt="Elastic Agent Builder New Agent" />

---

## The Agent Swarm

QuantumState uses four native Elastic Agent Builder agents â€” each responsible for a single stage of the incident lifecycle, each equipped with purpose-built ES|QL tools.

<img src="images/Web - The 4 Agents.png" width="720" alt="The 4 Agents" />

### ğŸ”­ Cassandra â€” Detect

Continuously monitors system metrics using rolling time windows. Instead of relying on static thresholds, it compares current behavior against a dynamic baseline to detect gradual degradation â€” memory leaks, error spikes, latency drift â€” before they escalate into critical failures. Returns anomaly type, confidence score, and time-to-critical estimate.

**Tools:** `detect_memory_leak` Â· `detect_error_spike` Â· `calculate_time_to_failure`

### ğŸ” Archaeologist â€” Investigate

Takes the anomaly context and correlates it with surrounding signals â€” logs, recent deployment events, and related system activity. Rather than identifying symptoms in isolation, it constructs an evidence chain linking cause to effect.

**Tools:** `search_error_logs` Â· `correlate_deployments` Â· `find_similar_incidents`

### âš•ï¸ Surgeon â€” Resolve

Evaluates possible remediation actions based on the detected anomaly and confidence score. Samples current service state, logs the intended action, then â€” if confidence â‰¥ 0.8 â€” calls `quantumstate.autonomous_remediation` directly to trigger the Kibana Workflow. The Workflow creates an audit Case and queues the action for the MCP Runner. Recovery verification is left to Guardian.

**Tools:** `log_remediation_action` Â· `get_recent_anomaly_metrics` Â· `verify_resolution` Â· `quantumstate.autonomous_remediation`

### ğŸ›¡ï¸ Guardian â€” Verify

Closes the loop. After remediation, it validates whether system health has returned to baseline â€” checking memory, error rate, and latency thresholds. Returns `RESOLVED` or `ESCALATE` with a calculated MTTR. Only when recovery is confirmed does the incident lifecycle complete.

**Tools:** `get_recent_anomaly_metrics` Â· `verify_resolution` Â· `get_incident_record` Â· `get_remediation_action`

---

## The MCP Runner

The MCP Runner is the component that physically executes remediation. It acts as a lightweight sidecar that continuously polls for approved remediation actions written by the agents to Elasticsearch.

When an action is marked ready for execution, the MCP Runner performs the required infrastructure operation â€” restarting a container, triggering a rollback, scaling a cache dependency.

- No webhooks
- No external orchestration engines
- No separate automation platform

Elasticsearch acts as the coordination layer and message bus. The MCP Runner bridges agent decisions with real-world execution, keeping the architecture simple, auditable, and fully controlled within the Elastic ecosystem.

---

## Architecture & Pipeline Flow

At a high level, the flow is:

1. Metrics and logs stream continuously into Elasticsearch.
2. The Agent Pipeline orchestrates the four specialized agents.
3. When remediation is approved (confidence â‰¥ 0.8), an Elastic Workflow is triggered.
4. The Workflow records the action and maintains an auditable trail.
5. The MCP Runner executes the infrastructure action.
6. Guardian verifies recovery and closes the incident.

> Detection â†’ Root Cause â†’ Remediation â†’ Verification â†’ Closure

<img src="images/architecture-flow.svg" width="720" alt="Architecture Flow" />

---

## Setup

### Prerequisites

- Python 3.12+ Â· Node.js 18+
- Docker (for the real infrastructure demo)
- Elastic Cloud deployment

### Step 1: Elastic Cloud

Start with a free [14-day Elastic Cloud trial](https://cloud.elastic.co). Once provisioned, create an API key in Kibana and copy your Cloud ID.

Create a `.env` file in the project root:

```env
ELASTIC_CLOUD_ID=My_Project:base64encodedstring==
ELASTIC_API_KEY=your_api_key_here==
```

The Kibana URL is derived automatically from the Cloud ID. You'll add `REMEDIATION_WORKFLOW_ID` after the next step.

Then enable both features in **Stack Management â†’ Advanced Settings**:

- `workflows:ui:enabled` â€” Elastic Workflows
- `agentBuilder:experimentalFeatures` â€” Elastic Agent Builder

This is a one-time step. Without it, the workflow deploy and agent setup will fail.

### Step 2: Deploy the Remediation Workflow

The workflow must exist before agents are created â€” the Surgeon agent requires its ID.

```bash
python elastic-setup/workflows/deploy_workflow.py
```

The script deploys `elastic-setup/workflows/remediation-workflow.yaml` to Kibana and prints the created workflow ID. Add it to `.env`:

```env
REMEDIATION_WORKFLOW_ID=workflow-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

Alternatively, create the workflow manually in the Kibana UI by importing `elastic-setup/workflows/remediation-workflow.yaml`.

### Step 3: Create Agents and Tools

```bash
python elastic-setup/setup_agents.py
```

Creates all 12 ES|QL tools and all 4 agents via the Kibana API in a single run. Idempotent â€” safe to re-run if you update instructions or tools.

```
â”€â”€ Step 1: Upsert 12 tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… detect_memory_leak                    [created]
  âœ… detect_error_spike                    [created]
  âœ… calculate_time_to_failure             [created]
  âœ… search_error_logs                     [created]
  âœ… correlate_deployments                 [created]
  âœ… find_similar_incidents                [created]
  âœ… log_remediation_action                [created]
  âœ… verify_resolution                     [created]
  âœ… get_recent_anomaly_metrics            [created]
  âœ… get_incident_record                   [created]
  âœ… get_remediation_action                [created]
  âœ… quantumstate.autonomous_remediation   [created]

â”€â”€ Step 2: Upsert 4 agents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… cassandra-detection-agent             [created]
  âœ… archaeologist-investigation-agent     [created]
  âœ… surgeon-action-agent                  [created]
  âœ… guardian-verification-agent           [created]
```

If you prefer to set up agents manually, every agent ID, system prompt, tool assignment, and ES|QL query is documented in [`agents-definition.md`](agents-definition.md).

> **Verify in Kibana after setup.** Once the script completes, open Kibana â†’ Agent Builder and confirm that all 4 agents appear with the correct tools assigned to each. Use [`agents-definition.md`](agents-definition.md) as the reference â€” it lists every agent's name, system prompt, and exact tool assignments. If anything looks wrong (missing tool, wrong prompt, incorrect ES|QL), edit it directly in the Kibana UI rather than re-running the script, as the UI gives you immediate feedback on what changed.

To tear everything down:

```bash
python elastic-setup/setup_agents.py --delete
```

<img src="images/Elastic Agent Builder - Agents List.png" width="720" alt="Elastic Agent Builder Agents List" />

### Step 4: Start the Application

```bash
git clone https://github.com/padmanabhan-r/QuantumState
cd QuantumState
uv sync
./start.sh
```

Launches the FastAPI backend on `http://localhost:8000` and the React frontend on `http://localhost:8080`.

---

## Running the Pipeline

### SRE Console

The Console is the main interface for running and observing the pipeline. Toggle **Auto Pipeline** on to run the full agent chain automatically on a schedule (90s locally, 3â€“5 min in production), or click **Run Pipeline** to trigger it immediately. Each agent's reasoning streams live to the terminal as it runs.

<!-- IMAGE: SRE Console screenshot -->

### Sim Control

Sim Control lets you manage the synthetic simulation environment without the Docker stack â€” set up indices, stream synthetic metrics, inject anomalies, and run the MCP Runner synthetically. Useful for quick testing without containers.

<!-- IMAGE: Sim Control screenshot -->

---

## Injecting Real Faults (Recommended)

The `infra/` directory contains a complete local microservice environment wired together via Docker Compose. Running this stack means the data Cassandra sees is real â€” actual memory allocation climbing inside a container, actual error logs being written, and an actual `docker restart` bringing memory back down.

```bash
cd infra
docker compose up --build
```

| Container | Port | Purpose |
|---|---|---|
| `payment-service` | 8001 | FastAPI service â€” memory leak target |
| `checkout-service` | 8002 | FastAPI service |
| `auth-service` | 8003 | FastAPI service â€” error spike target |
| `inventory-service` | 8004 | FastAPI service |
| `auth-redis` | 6379 | Redis dependency |
| `qs-scraper` | â€” | Polls `/health` every 15s â†’ writes to `metrics-quantumstate` |
| `qs-mcp-runner` | â€” | Polls `remediation-actions-quantumstate` every 0.5s â†’ `docker restart` |

Once up, the scraper immediately starts writing real readings to Elasticsearch. Cassandra has live data to work with.

#### Inject a fault

Use the TUI control panel:

```bash
uv run python infra/control.py
```

Press `1` to inject a memory leak into `payment-service`, `2` for an error spike into `auth-service`, `0` to reset everything.

Or via curl:

```bash
curl -X POST http://localhost:8001/simulate/leak
curl -X POST http://localhost:8003/simulate/spike?duration=600
curl -X POST http://localhost:8001/simulate/reset
```

#### What actually happens

When you inject a memory leak, `payment-service` allocates **4MB every 5 seconds** in real Python heap â€” not simulated. The scraper writes the rising readings to `metrics-quantumstate`. After ~30 seconds, the container starts emitting error logs:

```
ERROR HEAP_PRESSURE: JVM heap elevated: 58% â€” connection pool under pressure
WARN GC_OVERHEAD: GC overhead limit approaching: 63% heap utilised
CRITICAL OOM_IMMINENT: Out-of-memory condition imminent: 71% heap, GC unable to reclaim
```

These are the logs Archaeologist finds and builds its evidence chain from.

When Surgeon triggers remediation, the MCP Runner runs `docker restart payment-service`. The container restarts in 2â€“5 seconds. Memory drops back to baseline. The scraper writes the recovered readings. Guardian sees real recovery metrics.

The whole loop â€” memory climbing, detection, restart, recovery â€” is observable in real infrastructure.

#### Recommended trigger sequence

1. Start the Docker stack (`docker compose up --build` in `infra/`)
2. Wait ~2 minutes for baseline metrics to accumulate
3. Inject a fault via the TUI (`uv run python infra/control.py`)
4. Wait ~60â€“90 seconds for the fault to appear in the metrics index
5. Open `http://localhost:8080` â†’ Console â†’ **Run Pipeline**

---

## Demo

Here's the full pipeline running against a real memory leak injected into `payment-service`:

<!-- VIDEO: Full pipeline demo -->

1. Memory leak injected â€” `payment-service` allocates 4MB every 5s, memory climbs from ~42% to ~74%
2. Scraper writes real `/health` readings to `metrics-quantumstate` every 15s
3. Cassandra detects the deviation, calculates ~18 minutes to critical threshold
4. Archaeologist finds three correlated `HEAP_PRESSURE` and `OOM_IMMINENT` log entries
5. Surgeon evaluates confidence (0.91) â€” calls `quantumstate.autonomous_remediation` directly, triggering the Elastic Workflow
6. The Workflow creates a Kibana Case and writes the action to `remediation-actions-quantumstate` â€” the MCP Runner picks up the `pending` action within 0.5s and runs `docker restart payment-service`
7. Container restarts in ~3 seconds, memory drops to ~41%
8. Guardian verifies recovery against real post-restart metrics â†’ **RESOLVED. MTTR: ~3m 48s**

The entire incident â€” real memory allocation, real container restart, real recovery â€” runs end-to-end without any human input.

---

## Tech Stack

| Layer | Technology |
|---|---|
| Agent runtime | Elastic Agent Builder (Kibana) |
| Agent tools | ES\|QL â€” 12 custom parameterised queries |
| Workflow automation | Elastic Workflows (YAML, deployed via API) |
| Orchestration | Python FastAPI â€” SSE streaming |
| Data store | Elasticsearch Cloud |
| Frontend | React + Vite + TypeScript + shadcn/ui |

---

## Elasticsearch Indices

| Index | Purpose |
|---|---|
| `metrics-quantumstate` | Time-series CPU, memory, error rate, latency |
| `logs-quantumstate` | Application logs and deployment events |
| `incidents-quantumstate` | Full incident lifecycle records |
| `agent-decisions-quantumstate` | Agent decision audit trail |
| `remediation-actions-quantumstate` | Action queue polled by the MCP Runner |
| `remediation-results-quantumstate` | Guardian verdicts and post-fix metric readings |

All indices are created automatically the first time a document is written to them.

---

## Project Structure

```
quantumstate/
â”œâ”€â”€ frontend/                   React + Vite + TypeScript UI
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ pages/              Index, Console, SimControl
â”‚       â””â”€â”€ components/         console/, landing/, ui/
â”œâ”€â”€ backend/                    FastAPI Python backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ elastic.py              Shared ES client
â”‚   â”œâ”€â”€ orchestrator.py         Agent Builder SSE streaming
â”‚   â””â”€â”€ routers/
â”‚       â”œâ”€â”€ pipeline.py         4-agent orchestration
â”‚       â”œâ”€â”€ guardian.py         Post-remediation verification
â”‚       â”œâ”€â”€ remediate.py        Recovery metric writes
â”‚       â”œâ”€â”€ sim.py              Simulation control
â”‚       â”œâ”€â”€ incidents.py        Incident feed + MTTR stats
â”‚       â””â”€â”€ health.py           Live service health
â”œâ”€â”€ elastic-setup/
â”‚   â”œâ”€â”€ setup_agents.py         One-shot agent + tool provisioning
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ remediation-workflow.yaml
â”‚       â””â”€â”€ deploy_workflow.py
â”œâ”€â”€ infra/                      Real Docker microservice environment
â”‚   â”œâ”€â”€ services/               4 FastAPI services
â”‚   â”œâ”€â”€ scraper/                Metrics scraper
â”‚   â”œâ”€â”€ mcp-runner/             Real Docker remediation runner
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ agents-definition.md        Full Kibana setup reference
â”œâ”€â”€ start.sh                    Starts frontend + backend
â””â”€â”€ .env                        Elastic credentials (not committed)
```
