# QuantumState

ğŸŒ [Website](https://www.quantumstate.online) Â· ğŸ¤– [Agents Definition](agents-definition.md)

---

## Inspiration

Incident response today is a three-front problem.

The SRE wakes up at 2 AM to a memory leak. By the time they SSH in, scrape logs, correlate deployment timelines, and find the right runbook, 30 critical minutes are gone.

The AI engineer tasked with automating this hits integration hell. Building agentic workflows on top of Elasticsearch historically meant stitching together LangChain, a vector store, external LLM APIs, and custom orchestration logic into a fragile, high-latency chain.

The security engineer is quietly panicking. Every external AI integration means sensitive production telemetry leaving the cluster for third-party APIs.

We realised the intelligence couldn't sit on top of the data. It had to live inside it.

---

## What It Does

QuantumState is a self-healing, in-cluster incident response system powered by a four-agent SRE swarm:

- **Cassandra (Detection):** Detects anomalies using parameterised ES|QL queries, comparing peak values against a rolling baseline to catch progressive failures within minutes of onset.
- **Archaeologist (Investigation):** Uses ELSER-powered semantic search to correlate logs and surface similar historical incidents by meaning, not just keywords.
- **Surgeon (Remediation):** Retrieves the relevant runbook via semantic search and executes remediation through a Kibana Workflow when confidence is high enough.
- **Guardian (Verification):** Runs post-fix validation to confirm the cluster has stabilised before closing the incident.

---

## How We Built It

The swarm runs entirely inside Elastic using Agent Builder. All 13 tools and 4 agents are provisioned programmatically via the Kibana API, making the full setup reproducible from a single script. Tools are defined as parameterised ES|QL queries and Index Search calls, giving agents native, low-latency access to production telemetry.

A lightweight Python FastAPI backend orchestrates the four-agent pipeline over Server-Sent Events, invoking each agent through the Kibana API's converse endpoint and streaming progress to the frontend in real time. All agent logic and credentials remain inside the cluster. No external LLM APIs. No data exfiltration.

---

## Four agents, one closed loop

```
Elasticsearch (metrics + logs)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“¡ Cassandra        â”‚  Detection â€” ES|QL anomaly scan, time-to-failure forecast
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”¬ Archaeologist    â”‚  Investigation â€” log search, deployment correlation, historical match
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ©º Surgeon          â”‚  Remediation â€” runbook retrieval, triggers Elastic Workflow
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚  (autonomous if confidence â‰¥ 0.8)
           â–¼
  âš¡ Remediation executes
  Kibana Case created
  Recovery metrics written
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ›¡ï¸ Guardian         â”‚  Verification â€” post-fix metric check, MTTR calc, RESOLVED/ESCALATE
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
  incidents-quantumstate (closed incident record with MTTR)
```

All four agents are **native [Elastic Agent Builder](https://www.elastic.co/docs/explore-analyze/ai-features/agent-builder/agent-builder-agents) agents** â€” no external LLM API keys, no external orchestration framework. Everything runs inside your Elastic cluster.

<img src="images/Web - The 4 Agents.png" width="720" alt="The 4 Agents" />

---

## The Agent Swarm

### ğŸ”­ Cassandra: Detect

Continuously monitors system metrics using rolling time windows. Instead of relying on static thresholds, it compares current behavior against a dynamic baseline to detect gradual degradation â€” memory leaks, error spikes, latency drift â€” before they escalate into critical failures. Returns anomaly type, confidence score, and time-to-critical estimate.

**Tools:** `detect_memory_leak` Â· `detect_error_spike` Â· `calculate_time_to_failure`

### ğŸ” Archaeologist: Investigate

Takes the anomaly context and correlates it with surrounding signals â€” logs, recent deployment events, and historical incidents. Rather than identifying symptoms in isolation, it constructs an evidence chain linking cause to effect. The `find_similar_incidents` tool uses ELSER-powered hybrid search to surface semantically similar past incidents, even when described in completely different language.

**Tools:** `search_error_logs` Â· `correlate_deployments` Â· `find_similar_incidents`

### âš•ï¸ Surgeon: Resolve

Evaluates possible remediation actions based on the detected anomaly and confidence score. Samples current service state, retrieves the most relevant runbook from a semantically searchable procedure library, logs the intended action, then â€” if confidence â‰¥ 0.8 â€” calls `quantumstate.autonomous_remediation` directly to trigger the Kibana Workflow. The Workflow creates an audit Case and queues the action for the MCP Runner. Recovery verification is left to Guardian.

**Tools:** `get_recent_anomaly_metrics` Â· `find_relevant_runbook` Â· `log_remediation_action` Â· `verify_resolution` Â· `quantumstate.autonomous_remediation`

### ğŸ›¡ï¸ Guardian: Verify

Closes the loop. After remediation, it validates whether system health has returned to baseline â€” checking memory, error rate, and latency thresholds. Returns `RESOLVED` or `ESCALATE` with a calculated MTTR. Only when recovery is confirmed does the incident lifecycle complete.

**Tools:** `get_recent_anomaly_metrics` Â· `verify_resolution` Â· `get_incident_record` Â· `get_remediation_action`

---

## The MCP Runner

The MCP Runner is the component that physically executes remediation. It acts as a lightweight sidecar that continuously polls for approved remediation actions written by the agents to Elasticsearch.

When an action is marked ready for execution, the MCP Runner performs the required infrastructure operation: restarting a container, triggering a rollback, or scaling a cache dependency.

- No webhooks
- No external orchestration engines
- No separate automation platform

Elasticsearch acts as the coordination layer and message bus. The MCP Runner bridges agent decisions with real-world execution, keeping the architecture simple, auditable, and fully controlled within the Elastic ecosystem.

---

## Architecture & Pipeline Flow

At a high level, the flow is:

1. Metrics and logs stream continuously into Elasticsearch.
2. The Agent Pipeline orchestrates the four specialized agents.
3. When remediation is approved (confidence â‰¥ 0.8), an Elastic Workflow is triggered.
4. The Workflow records the action and maintains an auditable trail.
5. The MCP Runner executes the infrastructure action.
6. Guardian verifies recovery and closes the incident.

> Detection â†’ Root Cause â†’ Remediation â†’ Verification â†’ Closure

<img src="images/architecture-flow.svg" width="720" alt="Architecture Flow" />

---

## Setup

### Prerequisites

- Python 3.12+ Â· Node.js 18+
- Docker (for the real infrastructure demo)
- Elastic Cloud deployment

```bash
git clone https://github.com/padmanabhan-r/QuantumState.git
cd QuantumState
```

### Step 1: Elastic Cloud

Start with a free [14-day Elastic Cloud trial](https://cloud.elastic.co). Once provisioned:

1. From the Elastic Cloud home page, find the **Connection details** section on your deployment and click **Create API key** â€” copy the key once generated.
2. In the same panel, open the **Endpoints** tab and toggle **Show Cloud ID** â€” copy that value too.

Create a `.env` file in the project root with both values:

```env
ELASTIC_CLOUD_ID=My_Project:base64encodedstring==
ELASTIC_API_KEY=your_api_key_here==
```

The Kibana URL is derived automatically from the Cloud ID. You'll add `REMEDIATION_WORKFLOW_ID` after the next step.

Then enable both features in **Stack Management â†’ Advanced Settings**:

- `workflows:ui:enabled` â€” Elastic Workflows
- `agentBuilder:experimentalFeatures` â€” Elastic Agent Builder

This is a one-time step. Without it, the workflow deploy and agent setup will fail.

### Step 2: Deploy ELSER (Elastic Learned Sparse Encoder)

QuantumState uses ELSER for semantic search across historical incidents and runbooks. Deploy it once:

```bash
python elastic-setup/setup_elser.py
```

This creates the `.elser-2-elasticsearch` inference endpoint on your cluster. If ELSER is already deployed, the script detects this and exits immediately. This step is required before creating agents â€” two of the tools (`find_similar_incidents` and `find_relevant_runbook`) use Index Search against ELSER-indexed data, and Kibana validates the indices exist at tool creation time.

### Step 3: Deploy the Remediation Workflow

The workflow must exist before agents are created â€” the Surgeon agent requires its ID.

```bash
python elastic-setup/workflows/deploy_workflow.py
```

The script deploys `elastic-setup/workflows/remediation-workflow.yaml` to Kibana and prints the created workflow ID. Add it to `.env`:

```env
REMEDIATION_WORKFLOW_ID=workflow-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

Alternatively, create the workflow manually in the Kibana UI by importing `elastic-setup/workflows/remediation-workflow.yaml`.

### Step 4: Start the Application and Seed Data

```bash
./start.sh
```

Once running, open `http://localhost:8080` â†’ **Simulation & Setup â†’ Run Setup**. This creates all 7 Elasticsearch indices â€” including `incidents-quantumstate` and `runbooks-quantumstate` with their ELSER `semantic_text` field mappings â€” and seeds 100 historical incidents and 8 runbooks in a single pass. Both are required before the next step, as Kibana validates those indices exist at tool creation time.

### Step 5: Create Agents and Tools

```bash
python elastic-setup/setup_agents.py
```

Creates all 13 tools and 4 agents via the Kibana API in a single run. Idempotent â€” safe to re-run if you update instructions or tools.

```
â”€â”€ Step 1: Upsert 13 tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… detect_memory_leak                    [created]
  âœ… detect_error_spike                    [created]
  âœ… calculate_time_to_failure             [created]
  âœ… search_error_logs                     [created]
  âœ… correlate_deployments                 [created]
  âœ… find_similar_incidents                [created]
  âœ… find_relevant_runbook                 [created]
  âœ… log_remediation_action                [created]
  âœ… verify_resolution                     [created]
  âœ… get_recent_anomaly_metrics            [created]
  âœ… get_incident_record                   [created]
  âœ… get_remediation_action                [created]
  âœ… quantumstate.autonomous_remediation   [created]

â”€â”€ Step 2: Upsert 4 agents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… cassandra-detection-agent             [created]
  âœ… archaeologist-investigation-agent     [created]
  âœ… surgeon-action-agent                  [created]
  âœ… guardian-verification-agent           [created]
```

If you prefer to set up agents manually, every agent ID, system prompt, tool assignment, and query is documented in [`agents-definition.md`](agents-definition.md).

> **Verify in Kibana after setup.** Once the script completes, open Kibana â†’ Agent Builder and confirm that all 4 agents appear with the correct tools assigned to each. Use [`agents-definition.md`](agents-definition.md) as the reference â€” it lists every agent's name, system prompt, and exact tool assignments. If anything looks wrong (missing tool, wrong prompt, incorrect ES|QL), edit it directly in the Kibana UI rather than re-running the script, as the UI gives you immediate feedback on what changed.

To tear everything down:

```bash
python elastic-setup/setup_agents.py --delete
```

<img src="images/Elastic Agent Builder - Agents List.png" width="720" alt="Elastic Agent Builder Agents List" />

---

## Injecting Real Faults (Recommended)

The `infra/` directory contains a complete local microservice environment wired together via Docker Compose. Running this stack means the data Cassandra sees is real â€” actual memory allocation climbing inside a container, actual error logs being written, and an actual `docker restart` bringing memory back down.

```bash
cd infra
docker compose up --build
```

| Container | Port | Purpose |
|---|---|---|
| `payment-service` | 8001 | FastAPI service â€” memory leak target |
| `checkout-service` | 8002 | FastAPI service |
| `auth-service` | 8003 | FastAPI service â€” error spike target |
| `inventory-service` | 8004 | FastAPI service |
| `auth-redis` | 6379 | Redis dependency |
| `qs-scraper` | - | Polls `/health` every 15s, writes to `metrics-quantumstate` |
| `qs-mcp-runner` | - | Polls `remediation-actions-quantumstate` every 0.5s, runs `docker restart` |

Once up, the scraper immediately starts writing real readings to Elasticsearch. Cassandra has live data to work with.

#### Inject a fault

Use the TUI control panel:

```bash
uv run python infra/control.py
```

Press `1` to inject a memory leak into `payment-service`, `2` for an error spike into `auth-service`, `0` to reset everything.

Or via curl:

```bash
curl -X POST http://localhost:8001/simulate/leak
curl -X POST http://localhost:8003/simulate/spike?duration=600
curl -X POST http://localhost:8001/simulate/reset
```

#### What actually happens

When you inject a memory leak, `payment-service` allocates **4MB every 5 seconds** in real Python heap â€” not simulated. The scraper writes the rising readings to `metrics-quantumstate`. After ~30 seconds, the container starts emitting error logs:

```
ERROR HEAP_PRESSURE: JVM heap elevated: 58% â€” connection pool under pressure
WARN GC_OVERHEAD: GC overhead limit approaching: 63% heap utilised
CRITICAL OOM_IMMINENT: Out-of-memory condition imminent: 71% heap, GC unable to reclaim
```

These are the logs Archaeologist finds and builds its evidence chain from.

When Surgeon triggers remediation, the MCP Runner runs `docker restart payment-service`. The container restarts in 2â€“5 seconds. Memory drops back to baseline. The scraper writes the recovered readings. Guardian sees real recovery metrics.

The whole loop â€” memory climbing, detection, restart, recovery â€” is observable in real infrastructure.

#### Recommended trigger sequence

1. Start the Docker stack (`docker compose up --build` in `infra/`)
2. Wait ~2 minutes for baseline metrics to accumulate
3. Inject a fault via the TUI (`uv run python infra/control.py`)
4. Wait ~60â€“90 seconds for the fault to appear in the metrics index
5. Open `http://localhost:8080` â†’ Console â†’ **Run Pipeline**

---

## Running the Pipeline

### SRE Console

Click **Run Pipeline** from the Console tab to invoke the full four-agent chain. Each agent's reasoning streams live as it runs. Toggle **Auto Pipeline** to run automatically on a schedule.

<img src="images/Console and TUI.png" width="720" alt="SRE Console and TUI" />

### Simulation & Setup

No Docker? The Simulation & Setup page lets you manage the full environment from the browser â€” create indices, seed data, inject synthetic anomalies, and run the MCP Runner in-process without any containers.

<img src="images/Sim Control.png" width="720" alt="Simulation and Setup" />

---

## Demo

Here's the full pipeline running against a real memory leak injected into `payment-service`:

<!-- VIDEO: Full pipeline demo -->

1. Memory leak injected â€” `payment-service` allocates 4MB every 5s, memory climbs from ~42% to ~74%
2. Scraper writes real `/health` readings to `metrics-quantumstate` every 15s
3. Cassandra detects the deviation, calculates ~18 minutes to critical threshold
4. Archaeologist finds three correlated `HEAP_PRESSURE` and `OOM_IMMINENT` log entries
5. Surgeon evaluates confidence (0.91) â€” calls `quantumstate.autonomous_remediation` directly, triggering the Elastic Workflow
6. The Workflow creates a Kibana Case and writes the action to `remediation-actions-quantumstate` â€” the MCP Runner picks up the `pending` action within 0.5s and runs `docker restart payment-service`
7. Container restarts in ~3 seconds, memory drops to ~41%
8. Guardian verifies recovery against real post-restart metrics â†’ **RESOLVED. MTTR: ~3m 48s**

The entire incident â€” real memory allocation, real container restart, real recovery â€” runs end-to-end without any human input.

---

## Tech Stack

| Layer | Technology |
|---|---|
| Agent runtime | Elastic Agent Builder (Kibana) |
| Agent tools | 11 ES\|QL tools + 2 ELSER Index Search tools + 1 Workflow tool â€” 13 total |
| Workflow automation | Elastic Workflows (YAML, deployed via API) |
| Orchestration | Python FastAPI â€” SSE streaming |
| Data store | Elasticsearch Cloud |
| Frontend | React + Vite + TypeScript + shadcn/ui |

---

## Elasticsearch Indices

| Index | Purpose |
|---|---|
| `metrics-quantumstate` | Time-series CPU, memory, error rate, latency |
| `logs-quantumstate` | Application logs and deployment events |
| `incidents-quantumstate` | Full incident lifecycle records |
| `agent-decisions-quantumstate` | Agent decision audit trail |
| `remediation-actions-quantumstate` | Action queue polled by the MCP Runner |
| `remediation-results-quantumstate` | Guardian verdicts and post-fix metric readings |
| `runbooks-quantumstate` | Semantically searchable remediation procedure library (ELSER) |

Indices are created by **Simulation & Setup â†’ Run Setup** (Step 4). The `incidents-quantumstate` and `runbooks-quantumstate` indices require explicit creation with ELSER `semantic_text` field mappings and cannot be auto-created on first write.

---

## Project Structure

```
quantumstate/
â”œâ”€â”€ frontend/                   React + Vite + TypeScript UI
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ pages/              Index, Console, SimControl
â”‚       â””â”€â”€ components/         console/, landing/, ui/
â”œâ”€â”€ backend/                    FastAPI Python backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ elastic.py              Shared ES client
â”‚   â”œâ”€â”€ orchestrator.py         Agent Builder SSE streaming
â”‚   â””â”€â”€ routers/
â”‚       â”œâ”€â”€ pipeline.py         4-agent orchestration
â”‚       â”œâ”€â”€ guardian.py         Post-remediation verification
â”‚       â”œâ”€â”€ remediate.py        Recovery metric writes
â”‚       â”œâ”€â”€ sim.py              Simulation control
â”‚       â”œâ”€â”€ incidents.py        Incident feed + MTTR stats
â”‚       â””â”€â”€ health.py           Live service health
â”œâ”€â”€ elastic-setup/
â”‚   â”œâ”€â”€ setup_elser.py          ELSER inference endpoint deployment (one-time)
â”‚   â”œâ”€â”€ setup_agents.py         One-shot agent + tool provisioning
â”‚   â”œâ”€â”€ seed_runbooks.py        Runbook library seeder (8 runbooks)
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ remediation-workflow.yaml
â”‚       â””â”€â”€ deploy_workflow.py
â”œâ”€â”€ infra/                      Real Docker microservice environment
â”‚   â”œâ”€â”€ services/               4 FastAPI services
â”‚   â”œâ”€â”€ scraper/                Metrics scraper
â”‚   â”œâ”€â”€ mcp-runner/             Real Docker remediation runner
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ agents-definition.md        Full Kibana setup reference
â”œâ”€â”€ start.sh                    Starts frontend + backend
â””â”€â”€ .env                        Elastic credentials (not committed)
```
